{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import uniform, randint\n",
    "import json\n",
    "\n",
    "# Configuração inicial\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular métricas\n",
    "def calculate_metrics(y_true, y_pred, average='macro'):\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, average=average, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, average=average, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    }\n",
    "\n",
    "    print(f\"Acurácia: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Função genérica para busca de hiperparâmetros e avaliação\n",
    "def optimize_and_evaluate(model, param_dist, X, y, n_iter=20, cv_splits=5, scoring='f1_macro', model_name=\"Modelo\"):\n",
    "    # Validação cruzada estratificada\n",
    "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Configuração do RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Executar a busca\n",
    "    random_search.fit(X, y)\n",
    "    \n",
    "    # Armazenar histórico de desempenho\n",
    "    cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "    history = {\n",
    "        'mean_test_score': cv_results['mean_test_score'].tolist(),\n",
    "        'std_test_score': cv_results['std_test_score'].tolist(),\n",
    "        'params': cv_results['params'].tolist()\n",
    "    }\n",
    "    \n",
    "    # Melhor modelo\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    best_score = random_search.best_score_\n",
    "    \n",
    "    print(f\"\\n{model_name} - Melhores hiperparâmetros: {best_params}\")\n",
    "    print(f\"{model_name} - Melhor {scoring}: {best_score:.4f}\")\n",
    "    \n",
    "    return best_model, history\n",
    "\n",
    "# Função para plotar evolução dos resultados\n",
    "def plot_search_results(history_dict, metric='f1_macro'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for model_name, history in history_dict.items():\n",
    "        mean_scores = history['mean_test_score']\n",
    "        std_scores = history['std_test_score']\n",
    "        plt.plot(range(len(mean_scores)), mean_scores, label=f'{model_name} (média)')\n",
    "        plt.fill_between(range(len(mean_scores)), \n",
    "                        mean_scores - std_scores, \n",
    "                        mean_scores + std_scores, \n",
    "                        alpha=0.2)\n",
    "    plt.title(f'Evolução do {metric} durante a busca de hiperparâmetros')\n",
    "    plt.xlabel('Iteração')\n",
    "    plt.ylabel(f'{metric} (média)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Função para plotar boxplot das métricas\n",
    "def plot_metrics_boxplot(metrics_dict):\n",
    "    metrics_df = pd.DataFrame(metrics_dict).T\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=metrics_df)\n",
    "    plt.title('Comparação de Métricas entre Modelos')\n",
    "    plt.ylabel('Valor da Métrica')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# extraindo os dados separados de treino e teste\n",
    "\n",
    "X_train = pd.read_csv('data/X_train.csv')\n",
    "X_test = pd.read_csv('data/X_test.csv')\n",
    "y_train = pd.read_csv('data/y_train.csv')\n",
    "y_test = pd.read_csv('data/y_test.csv')\n",
    "\n",
    "y_train = np.ravel(y_train) # transformando em array\n",
    "y_test = np.ravel(y_test) # transformando em array\n",
    "\n",
    "# realizando a normalização dos dados\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionários para armazenar resultados\n",
    "history_dict = {}\n",
    "metrics_dict = {}\n",
    "best_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_knn = {\n",
    "    'n_neighbors': range(2, 21),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "params_decision_tree = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': list(range(3, 21, 2)),\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8]\n",
    "}\n",
    "\n",
    "params_random_forest = {\n",
    "    'n_estimators': range(1, 100, 5),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': list(range(3, 30, 3))\n",
    "}\n",
    "\n",
    "params_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'max_iter': [1000, 2500, 5000]\n",
    "}\n",
    "\n",
    "params_mlp = {\n",
    "    'hidden_layer_sizes': [(10,), (20,), (50,), (100,), (50, 50), (100, 50), (50, 25)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "params_xgboost = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth': range(3, 11),\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 1]\n",
    "}\n",
    "\n",
    "params_lightgbm = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': range(3, 11),\n",
    "    'num_leaves': [15, 31, 63],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando Decision Tree ...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "Decision Tree - Melhores hiperparâmetros: {'splitter': 'best', 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_depth': 9, 'criterion': 'gini'}\n",
      "Decision Tree - Melhor f1_macro: 0.4008\n",
      "Acurácia: 0.5894\n",
      "Precision: 0.5591\n",
      "Recall: 0.4318\n",
      "F1: 0.4227\n",
      "{'Decision Tree': {'mean_test_score': [0.39337197584237715, 0.4007625883773893, 0.37617969157242703, 0.39448744031769395, 0.3920826078462809, 0.3919780550408446, 0.39332952952476286, 0.3116133088305769, 0.38618303342569443, 0.3850422792984215, 0.3970059886252619, 0.38388297383980385, 0.3902047236294478, 0.3847099866489866, 0.39360891109099855, 0.3909722722892146, 0.3899648163102377, 0.31628968922940526, 0.3951737868695605, 0.37888876829318524], 'std_test_score': [0.0019505280708409303, 0.003039586784805576, 0.01053005974557641, 0.005237597153986394, 0.008921573475589847, 0.008910765459401714, 0.0039904392430780726, 0.025236653362989983, 0.006310672317399963, 0.004748027954678078, 0.006098583477235567, 0.0037164816941890892, 0.0048163633209686914, 0.003484977994296032, 0.006507807673914845, 0.0031933807107838302, 0.003356754813558099, 0.039177887128078216, 0.002753635782611341, 0.0085065222336845], 'params': [{'splitter': 'best', 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 17, 'criterion': 'gini'}, {'splitter': 'best', 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_depth': 9, 'criterion': 'gini'}, {'splitter': 'best', 'min_samples_split': 5, 'min_samples_leaf': 8, 'max_depth': 5, 'criterion': 'entropy'}, {'splitter': 'best', 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 15, 'criterion': 'entropy'}, {'splitter': 'best', 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 9, 'criterion': 'entropy'}, {'splitter': 'best', 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': 9, 'criterion': 'entropy'}, {'splitter': 'random', 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 19, 'criterion': 'gini'}, {'splitter': 'random', 'min_samples_split': 10, 'min_samples_leaf': 8, 'max_depth': 3, 'criterion': 'gini'}, {'splitter': 'best', 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 7, 'criterion': 'gini'}, {'splitter': 'best', 'min_samples_split': 10, 'min_samples_leaf': 8, 'max_depth': 7, 'criterion': 'entropy'}, {'splitter': 'best', 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_depth': 13, 'criterion': 'entropy'}, {'splitter': 'random', 'min_samples_split': 20, 'min_samples_leaf': 8, 'max_depth': 11, 'criterion': 'entropy'}, {'splitter': 'random', 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_depth': 19, 'criterion': 'gini'}, {'splitter': 'best', 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 7, 'criterion': 'entropy'}, {'splitter': 'best', 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_depth': 17, 'criterion': 'entropy'}, {'splitter': 'random', 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_depth': 13, 'criterion': 'gini'}, {'splitter': 'random', 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 19, 'criterion': 'gini'}, {'splitter': 'random', 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_depth': 5, 'criterion': 'gini'}, {'splitter': 'best', 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_depth': 17, 'criterion': 'entropy'}, {'splitter': 'random', 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_depth': 11, 'criterion': 'gini'}]}}\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model_name = 'Decision Tree'\n",
    "\n",
    "print(f\"\\nTreinando Decision Tree ...\")\n",
    "\n",
    "best_model, history = optimize_and_evaluate(\n",
    "    model=model,\n",
    "    param_dist=params_decision_tree,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    n_iter=20,\n",
    "    cv_splits=5,\n",
    "    scoring='f1_macro',\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "# Armazenar histórico e melhor modelo\n",
    "history_dict[model_name] = history\n",
    "best_models[model_name] = best_model\n",
    "\n",
    "# Avaliar métricas no conjunto completo\n",
    "y_pred = best_model.predict(X_train)\n",
    "\n",
    "metrics = calculate_metrics(y_train, y_pred, average='macro')\n",
    "metrics_dict[model_name] = metrics\n",
    "\n",
    "# trasforma o resultado e cria um arquivo json\n",
    "result = json.dumps(history_dict)\n",
    "\n",
    "with open(f'historico_busca_hiperparametros_{model_name}.json', 'w') as f:\n",
    "    f.write(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "model_name = 'KNN'\n",
    "\n",
    "print(f\"\\nTreinando KNN ...\")\n",
    "\n",
    "best_model, history = optimize_and_evaluate(\n",
    "    model=model,\n",
    "    param_dist=params_knn,\n",
    "    X=X_train,  # Substitua por seus dados\n",
    "    y=y_train,  # Substitua por seus dados\n",
    "    n_iter=20,\n",
    "    cv_splits=5,\n",
    "    scoring='f1_macro',\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "# Armazenar histórico e melhor modelo\n",
    "history_dict[model_name] = history\n",
    "best_models[model_name] = best_model\n",
    "\n",
    "# Avaliar métricas no conjunto completo\n",
    "y_pred = best_model.predict(X_train)\n",
    "y_prob = best_model.predict_proba(X_train) if hasattr(best_model, 'predict_proba') else None\n",
    "metrics = calculate_metrics(y_train, y_pred, y_prob, average='macro')\n",
    "metrics_dict[model_name] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualizações\n",
    "plot_search_results(history_dict, metric='F1-Score Macro')\n",
    "plot_metrics_boxplot(metrics_dict)\n",
    "\n",
    "# Exibir resultados finais\n",
    "print(\"\\nResultados Finais:\")\n",
    "for model_name, metrics in metrics_dict.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
